{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice importing and manipulating the SNOTEL data\n",
    "\n",
    "Website for report generator:https://wcc.sc.egov.usda.gov/reportGenerator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the strategy here for obtaining data is to select what type of data you want and in what time range through the Report Generator webtool. This gives you a URL specific to the type of report. Its interpretation is not simple and as such one cannot simply modify the URL to select different data types etc. However, the station ID is easy to change in the URL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "This URL takes you to a CSV of data from CA station \"Sonora Pass\" ID:771. It is saved as 'SonoraPass_1.csv' in this directory. You can see some obvious features of the URL: station ID, the state, and type of site (here \"SNTL\"). You can also see towards the end what columns are to be included and what data type is contained in the column (here all the raw values).\n",
    "\n",
    "https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customMultiTimeSeriesGroupByStationReport/monthly/start_of_period/771:CA:SNTL%7Cname=%22Sonora%20Paa%22%20AND%20outServiceDate=%222100-01-01%22%7Cname/-12,0/TAVG::value,SNWD::value,NTRDV::value,RHUM::value?fitToScreen=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Date' 'Sonora Pass (771) Air Temperature Average (degF)'\n",
      "  'Sonora Pass (771) Snow Depth (in) Start of Month Values'\n",
      "  'Sonora Pass (771) Net Solar Radiation Average (watt/m2)'\n",
      "  'Sonora Pass (771) Relative Humidity (pct) Start of Month Values']\n",
      " ['Oct 2019' '40' '1' '' '']\n",
      " ['Nov 2019' '36' '0' '' '']\n",
      " ['Dec 2019' '28' '13' '' '']\n",
      " ['Jan 2020' '30' '28' '' '']\n",
      " ['Feb 2020' '30' '28' '' '']\n",
      " ['Mar 2020' '28' '28' '' '']\n",
      " ['Apr 2020' '37' '41' '' '']\n",
      " ['May 2020' '45' '27' '' '']\n",
      " ['Jun 2020' '51' '0' '' '']\n",
      " ['Jul 2020' '58' '0' '' '']\n",
      " ['Aug 2020' '60' '0' '' '']\n",
      " ['Sep 2020' '55' '0' '' '']\n",
      " ['Oct 2020' '' '0' '' '']]\n"
     ]
    }
   ],
   "source": [
    "#Load in this data\n",
    "Data1 = np.loadtxt('SonoraPass_1.csv',skiprows=1,dtype='str',delimiter=',')\n",
    "\n",
    "print(Data1)\n",
    "\n",
    "#a bit annoying since the numeric values will need to be changed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try modifying the URL and getting other datasets. \n",
    "\n",
    "This page has all the SNOTEL stations listed (definitely extract this table later).\n",
    "https://wcc.sc.egov.usda.gov/nwcc/yearcount?network=sntl&counttype=statelist&state=\n",
    "\n",
    "Lets switch the station to the \"Monitor Pass\" station, ID=633"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "URLstart = 'https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customMultiTimeSeriesGroupByStationReport/monthly/start_of_period/'\n",
    "URLend = '%7Cname=%22Sonora%20Paa%22%20AND%20outServiceDate=%222100-01-01%22%7Cname/-12,0/TAVG::value,SNWD::value,NTRDV::value,RHUM::value?fitToScreen=false'\n",
    "\n",
    "URL_ID = '771:CA:SNTL'\n",
    "\n",
    "URL = URLstart + URL_ID + URLend\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think using the csv version is not quite as good maybe. You have to 1) save the csv from the webpage (there must be a way to do this in Python, but I don't know it at the moment), 2) the csv formatting is such that some post-processing needs to be done to get numeric values in float format.\n",
    "\n",
    "Let's try using the html version and using BeautifulSoup instead\n",
    "\n",
    "This is the Sonora Pass HTML URL:\n",
    "https://wcc.sc.egov.usda.gov/reportGenerator/view/customMultiTimeSeriesGroupByStationReport/monthly/start_of_period/771:CA:SNTL%7Cid=%22%22%7Cname/-12,0/TAVG::value,SNWD::value,SNRR::value?fitToScreen=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "####I had to add this is to get urlopen to work on my Mac\n",
    "import os, ssl\n",
    "if (not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None)):\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#####\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SP = 'https://wcc.sc.egov.usda.gov/reportGenerator/view/customMultiTimeSeriesGroupByStationReport/monthly/start_of_period/771:CA:SNTL%7Cid=%22%22%7Cname/-12,0/TAVG::value,SNWD::value,SNRR::value?fitToScreen=false'\n",
    "\n",
    "HTML_SP = urlopen(URL_SP)\n",
    "\n",
    "BS_SP = BeautifulSoup(HTML_SP,'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Av. Air temp Snow Depth Snow Perc.\n",
      "Oct 2019           40          1           \n",
      "Nov 2019           36          0      100.0\n",
      "Dec 2019           28         13       95.0\n",
      "Jan 2020           30         28       86.0\n",
      "Feb 2020           30         28       50.0\n",
      "Mar 2020           28         28      105.0\n",
      "Apr 2020           37         41       91.0\n",
      "May 2020           45         27       33.0\n",
      "Jun 2020           51          0        0.0\n",
      "Jul 2020           58          0        0.0\n",
      "Aug 2020           60          0        0.0\n",
      "Sep 2020           55          0        0.0\n",
      "Oct 2020           50          0           \n"
     ]
    }
   ],
   "source": [
    "#Now we have BS object for the page. Extract the table, the from the table sub-object extract\n",
    "    #rows. Then from the rows sub-object list exract the datapoints\n",
    "    \n",
    "Table = BS_SP.find_all('table',{'role':'grid'})\n",
    "#Table_Report = Table.find('tbody',{'id':'tabPanel:formReport:tblViewData_data'})\n",
    "ID = 'tabPanel:formReport:tblViewData_data' #this is the tbody \"id\" attribute for the Form\n",
    "    #Report table\n",
    "\n",
    "Ntable = len(Table)\n",
    "Ind_Report = []\n",
    "for ii in range(Ntable):\n",
    "    A = Table[ii].find('tbody',{\"id\":\"tabPanel:formReport:tblViewData_data\"})\n",
    "    if A is not None:\n",
    "         Ind_Report = ii\n",
    "\n",
    "Table_Report = Table[Ind_Report]\n",
    "\n",
    "#Now get all rows\n",
    "Rows = Table_Report.find_all('tr',{'role':'row'})\n",
    "Nrows = len(Rows)\n",
    "\n",
    "Ncol = len(Rows[0].find_all('td'))-1\n",
    "\n",
    "Date = [None]*Nrows\n",
    "for ii in range(Nrows):\n",
    "    Date[ii] = Rows[ii].find('td').text\n",
    "\n",
    "Cols = ['Av. Air temp','Snow Depth','Snow Perc.']\n",
    "Data_SP = pd.DataFrame(index=Date)\n",
    "for ii in range(Ncol):\n",
    "    Data_SP[Cols[ii]] = [None]*Nrows\n",
    "\n",
    "for ii in range(Nrows):\n",
    "    El = Rows[ii].find_all('td')\n",
    "    for jj in range(Ncol):\n",
    "        Data_SP.loc[Date[ii],Cols[jj]] = El[jj+1].text\n",
    "            \n",
    "print(Data_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should be able to change site ID and get all the same info for a new site\n",
    "\n",
    " \n",
    "ID = 633 \n",
    "State = 'CA'\n",
    "StationType = 'SNTL'\n",
    "\n",
    "URLstart = 'https://wcc.sc.egov.usda.gov/reportGenerator/view/customMultiTimeSeriesGroupByStationReport/monthly/start_of_period/'\n",
    "URLend = '%7Cid=%22%22%7Cname/-12,0/TAVG::value,SNWD::value,SNRR::value?fitToScreen=false' \n",
    "URLid = str(ID)+':'+State+':'+StationType\n",
    "        \n",
    "URL = URLstart+URLid+URLend\n",
    "\n",
    "HTML = urlopen(URL)\n",
    "\n",
    "BS = BeautifulSoup(HTML,'html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-574b2f692347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Av. Air temp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Snow Depth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Snow Perc.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mNrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dates' is not defined"
     ]
    }
   ],
   "source": [
    "Table = BS.find_all('table',{'role':'grid'})\n",
    "#Table_Report = Table.find('tbody',{'id':'tabPanel:formReport:tblViewData_data'})\n",
    "ID = 'tabPanel:formReport:tblViewData_data' #this is the tbody \"id\" attribute for the Form\n",
    "    #Report table\n",
    "\n",
    "Ntable = len(Table)\n",
    "Ind_Report = []\n",
    "for ii in range(Ntable):\n",
    "    A = Table[ii].find('tbody',{\"id\":\"tabPanel:formReport:tblViewData_data\"})\n",
    "    if A is not None:\n",
    "         Ind_Report = ii\n",
    "\n",
    "Table_Report = Table[Ind_Report]\n",
    "\n",
    "#Now get all rows\n",
    "Rows = Table_Report.find_all('tr',{'role':'row'})\n",
    "Nrows = len(Rows)\n",
    "\n",
    "Ncol = len(Rows[0].find_all('td'))-1\n",
    "\n",
    "Date = [None]*Nrows\n",
    "for ii in range(Nrows):\n",
    "    Date[ii] = Rows[ii].find('td').text\n",
    "\n",
    "Cols = ['Av. Air temp','Snow Depth','Snow Perc.']\n",
    "Data = pd.DataFrame(index=Date)\n",
    "for ii in range(Ncol):\n",
    "    Data[Cols[ii]] = [None]*Nrows\n",
    "\n",
    "for ii in range(Nrows):\n",
    "    El = Rows[ii].find_all('td')\n",
    "    for jj in range(Ncol):\n",
    "        Data.loc[Date[ii],Cols[jj]] = El[jj+1].text\n",
    "            \n",
    "print(Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to have a class that can extract certain types of reports given some inputs. It would also be useful to be able to extract site information (such as site IDs, elevation, lat and long, etc.) and have those saves in a database in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NRCS_report import report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep.SiteType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
